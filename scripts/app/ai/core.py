import os
from pathlib import Path
from typing import List, Optional
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document

class KnowledgeBase:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –æ—Ö—Ä–∞–Ω–µ —Ç—Ä—É–¥–∞."""
    
    def __init__(self, persist_directory: str = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π.
        
        Args:
            persist_directory: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î.
                              –ï—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è data/vector_db/
        """
        if persist_directory is None:
            # –ü—É—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞
            current_dir = Path(__file__).parent.parent.parent
            self.persist_directory = str(current_dir / "data" / "vector_db")
        else:
            self.persist_directory = persist_directory
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        os.makedirs(self.persist_directory, exist_ok=True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Ç–µ–∫—Å—Ç–∞)
        # 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2' —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å —Ä—É—Å—Å–∫–∏–º
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            model_kwargs={'device': 'cpu'},  # –ò—Å–ø–æ–ª—å–∑—É–µ–º CPU –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            encode_kwargs={'normalize_embeddings': True}
        )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        self.vectorstore = None
        self._load_existing_db()
    
    def _load_existing_db(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö, –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å."""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è –±–∞–∑–∞
            if os.path.exists(os.path.join(self.persist_directory, "chroma.sqlite3")):
                print(f"üìÇ –ó–∞–≥—Ä—É–∂–∞—é —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –∏–∑ {self.persist_directory}")
                self.vectorstore = Chroma(
                    persist_directory=self.persist_directory,
                    embedding_function=self.embeddings
                )
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {self.vectorstore._collection.count()}")
            else:
                print("üÜï –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è.")
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –±–∞–∑—É: {e}")
            print("–°–æ–∑–¥–∞—é –Ω–æ–≤—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π...")
    
    def load_documents(self, file_paths: List[Path]):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ —Ñ–∞–π–ª–æ–≤ –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –∏—Ö –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π.
        
        Args:
            file_paths: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Ç–µ–∫—Å—Ç–æ–≤—ã–º —Ñ–∞–π–ª–∞–º.
        """
        documents = []
        
        for file_path in file_paths:
            try:
                print(f"üìñ –ß–∏—Ç–∞—é —Ñ–∞–π–ª: {file_path.name}")
                
                # –ß–∏—Ç–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª
                with open(file_path, 'r', encoding='utf-8') as f:
                    text = f.read()
                
                if not text.strip():
                    print(f"  ‚ö†Ô∏è –§–∞–π–ª {file_path.name} –ø—É—Å—Ç, –ø—Ä–æ–ø—É—Å–∫–∞—é.")
                    continue
                
                # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç LangChain —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
                doc = Document(
                    page_content=text,
                    metadata={
                        "source": file_path.name,
                        "file_path": str(file_path),
                        "type": "law"  # –¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞: –∑–∞–∫–æ–Ω, –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –∏ —Ç.–¥.
                    }
                )
                documents.append(doc)
                print(f"  ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
                
            except Exception as e:
                print(f"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {file_path.name}: {e}")
        
        if not documents:
            print("‚ùå –ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
            return
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ —á–∞–Ω–∫–∏ (—Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã)
        print("\n‚úÇÔ∏è –†–∞–∑–±–∏–≤–∞—é –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã...")
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,      # –†–∞–∑–º–µ—Ä —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
            chunk_overlap=200,    # –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞–º–∏
            separators=["\n\n", "\n", " ", ""]  # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
        )
        
        chunks = text_splitter.split_documents(documents)
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∏–∑ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.")
        
        # –°–æ–∑–¥–∞–µ–º –∏–ª–∏ –æ–±–Ω–æ–≤–ª—è–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        if self.vectorstore is None:
            print("üÜï –°–æ–∑–¥–∞—é –Ω–æ–≤—É—é –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É...")
            self.vectorstore = Chroma.from_documents(
                documents=chunks,
                embedding=self.embeddings,
                persist_directory=self.persist_directory
            )
        else:
            print("üîÑ –î–æ–±–∞–≤–ª—è—é –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –±–∞–∑—É...")
            # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –±–∞–∑—ã
            # –ü–æ–∫–∞ –ø—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã
            self.vectorstore = Chroma.from_documents(
                documents=chunks,
                embedding=self.embeddings,
                persist_directory=self.persist_directory
            )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞–∑—É –Ω–∞ –¥–∏—Å–∫
        self.vectorstore.persist()
        print(f"üíæ –ë–∞–∑–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {self.persist_directory}")
    
    def search(self, query: str, k: int = 5) -> List:
        """
        –ò—â–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –ø–æ –∑–∞–ø—Ä–æ—Å—É.
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å.
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (–¥–æ–∫—É–º–µ–Ω—Ç, –æ—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏).
        """
        if self.vectorstore is None:
            print("‚ùå –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã.")
            return []
        
        try:
            # –ò—â–µ–º k –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
            results = self.vectorstore.similarity_search_with_relevance_scores(query, k=k)
            return results
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")
            return []
    
    def get_db_path(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î."""
        return self.persist_directory
    
    def get_document_count(self) -> int:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑–µ."""
        if self.vectorstore is None:
            return 0
        try:
            return self.vectorstore._collection.count()
        except:
            return 0

# –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥—É–ª—è KnowledgeBase...")
    
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    kb = KnowledgeBase()
    
    # –ü—É—Ç—å –∫ —Ç–µ—Å—Ç–æ–≤–æ–º—É –¥–æ–∫—É–º–µ–Ω—Ç—É
    test_doc = Path(__file__).parent.parent.parent / "data" / "texts" / "–¢—Ä—É–¥–æ–≤–æ–π –∫–æ–¥–µ–∫—Å –†–§.txt"
    
    if test_doc.exists():
        kb.load_documents([test_doc])
        
        # –¢–µ—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫
        test_query = "–æ—Ö—Ä–∞–Ω–∞ —Ç—Ä—É–¥–∞"
        print(f"\nüîç –¢–µ—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫: '{test_query}'")
        results = kb.search(test_query, k=2)
        
        for i, (doc, score) in enumerate(results, 1):
            print(f"\n{i}. [–°—Ö–æ–¥—Å—Ç–≤–æ: {score:.3f}]")
            print(f"   –ò—Å—Ç–æ—á–Ω–∏–∫: {doc.metadata.get('source', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
            print(f"   –¢–µ–∫—Å—Ç: {doc.page_content[:150]}...")
    else:
        print(f"‚ùå –¢–µ—Å—Ç–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {test_doc}")
        print("–ü–æ–º–µ—Å—Ç–∏—Ç–µ —Ñ–∞–π–ª '–¢—Ä—É–¥–æ–≤–æ–π –∫–æ–¥–µ–∫—Å –†–§.txt' –≤ data/texts/")
