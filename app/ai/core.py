import os
from pathlib import Path
from typing import List, Tuple, Optional
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ==================== –ë–ê–ó–û–í–´–ï –ö–õ–ê–°–°–´ ====================

class Document:
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏."""
    def __init__(self, page_content: str, metadata: Optional[dict] = None):
        self.page_content = page_content
        self.metadata = metadata or {}

# ==================== –£–ü–†–û–©–ï–ù–ù–ê–Ø –ë–ê–ó–ê –ó–ù–ê–ù–ò–ô ====================

class SimpleKnowledgeBase:
    """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –¥–ª—è —Ä–∞–±–æ—Ç—ã –±–µ–∑ LangChain."""
    
    def __init__(self, persist_directory: Optional[str] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π.
        
        Args:
            persist_directory: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –ø—Ä–æ—Å—Ç–æ–π –≤–µ—Ä—Å–∏–∏)
        """
        self.documents = []
        self.persist_directory = persist_directory or "./data/vector_db"
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        os.makedirs(self.persist_directory, exist_ok=True)
        logger.info("‚úÖ –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
    
    def load_documents(self, file_paths: List[Path]):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ —Ñ–∞–π–ª–æ–≤.
        
        Args:
            file_paths: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Ç–µ–∫—Å—Ç–æ–≤—ã–º —Ñ–∞–π–ª–∞–º
        """
        if not file_paths:
            logger.warning("‚ùå –ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏")
            return
        
        logger.info(f"üìö –ó–∞–≥—Ä—É–∂–∞—é {len(file_paths)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        
        loaded_count = 0
        for file_path in file_paths:
            try:
                # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    text = f.read().strip()
                
                if not text:
                    logger.warning(f"  ‚ö†Ô∏è –§–∞–π–ª –ø—É—Å—Ç: {file_path.name}")
                    continue
                
                # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏ (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
                chunks = self._split_text(text)
                
                for i, chunk in enumerate(chunks):
                    doc = Document(
                        page_content=chunk,
                        metadata={
                            "source": file_path.name,
                            "chunk": i,
                            "file_path": str(file_path)
                        }
                    )
                    self.documents.append(doc)
                
                loaded_count += 1
                logger.info(f"  ‚úÖ {file_path.name}: {len(chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
                
            except Exception as e:
                logger.error(f"  ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file_path.name}: {e}")
        
        logger.info(f"üìä –í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(self.documents)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∏–∑ {loaded_count} —Ñ–∞–π–ª–æ–≤")
    
    def _split_text(self, text: str, chunk_size: int = 1000) -> List[str]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã."""
        chunks = []
        start = 0
        text_length = len(text)
        
        while start < text_length:
            end = start + chunk_size
            
            # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞–∑–±–∏—Ç—å –ø–æ –≥—Ä–∞–Ω–∏—Ü–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            if end < text_length:
                for separator in ['. ', '.\n', '\n\n', '; ', '! ', '? ']:
                    pos = text.rfind(separator, start, end)
                    if pos != -1:
                        end = pos + len(separator)
                        break
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end
        
        return chunks
    
    def search(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:
        """
        –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º.
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (–¥–æ–∫—É–º–µ–Ω—Ç, –æ—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏)
        """
        if not self.documents:
            logger.warning("‚ùå –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø—É—Å—Ç–∞. –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã.")
            return []
        
        query_lower = query.lower()
        query_words = [word for word in query_lower.split() if len(word) > 2]
        
        if not query_words:
            logger.warning("‚ö†Ô∏è –ó–∞–ø—Ä–æ—Å —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π")
            return []
        
        results = []
        
        for doc in self.documents:
            content_lower = doc.page_content.lower()
            
            # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Å—á–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
            score = 0
            for word in query_words:
                if word in content_lower:
                    score += 1
            
            if score > 0:
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –æ—Ü–µ–Ω–∫—É
                normalized_score = min(score / len(query_words), 1.0)
                results.append((doc, normalized_score))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        results.sort(key=lambda x: x[1], reverse=True)
        
        logger.info(f"üîç –ü–æ –∑–∞–ø—Ä–æ—Å—É '{query}' –Ω–∞–π–¥–µ–Ω–æ {len(results)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
        return results[:k]
    
    def get_document_count(self) -> int:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑–µ."""
        return len(self.documents)
    
    def get_db_path(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –ë–î."""
        return self.persist_directory
    
    def create_vectorstore_from_documents(self):
        """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏."""
        logger.info("‚ÑπÔ∏è –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –≤ —É–ø—Ä–æ—â–µ–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏")
        return True

# ==================== –ê–õ–ò–ê–° –î–õ–Ø –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–ò ====================

# –û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
KnowledgeBase = SimpleKnowledgeBase

# ==================== –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï ====================

def test_knowledge_base():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π."""
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É–ø—Ä–æ—â–µ–Ω–Ω–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π...")
    print("=" * 60)
    
    kb = KnowledgeBase()
    print(f"‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å–æ–∑–¥–∞–Ω–∞")
    print(f"üìÅ –ü—É—Ç—å: {kb.get_db_path()}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    data_dir = Path(__file__).parent.parent.parent / "data" / "texts"
    
    if data_dir.exists():
        txt_files = list(data_dir.glob("*.txt"))
        print(f"\nüìö –ù–∞–π–¥–µ–Ω–æ {len(txt_files)} TXT —Ñ–∞–π–ª–æ–≤:")
        
        for file in txt_files:
            size_kb = file.stat().st_size / 1024 if file.exists() else 0
            print(f"  ‚Ä¢ {file.name} ({size_kb:.1f} –ö–ë)")
        
        if txt_files:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
            kb.load_documents(txt_files[:1])  # –¢–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π –¥–ª—è —Ç–µ—Å—Ç–∞
            
            # –¢–µ—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫
            test_queries = ["–æ—Ç–ø—É—Å–∫", "–æ—Ö—Ä–∞–Ω–∞ —Ç—Ä—É–¥–∞", "—Ä–∞–±–æ—á–µ–µ –≤—Ä–µ–º—è"]
            
            for query in test_queries:
                print(f"\nüîç –ü–æ–∏—Å–∫: '{query}'")
                results = kb.search(query, k=1)
                
                if results:
                    doc, score = results[0]
                    source = doc.metadata.get('source', '–î–æ–∫—É–º–µ–Ω—Ç')
                    preview = doc.page_content[:100].replace('\n', ' ')
                    print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ –≤: {source} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {score:.2f})")
                    print(f"   üìÑ –§—Ä–∞–≥–º–µ–Ω—Ç: {preview}...")
                else:
                    print("   ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ")
    else:
        print(f"\n‚ùå –ü–∞–ø–∫–∞ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {data_dir}")
        print("   –ü–æ–º–µ—Å—Ç–∏—Ç–µ —Ñ–∞–π–ª—ã .txt –≤ data/texts/")
    
    print("\n" + "=" * 60)
    print("‚úÖ –¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω!")

if __name__ == "__main__":
    test_knowledge_base()
