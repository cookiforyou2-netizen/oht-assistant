import os
import sys
from pathlib import Path
from typing import List, Optional, Tuple
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

try:
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import Chroma
    from langchain_community.embeddings import HuggingFaceEmbeddings
    from langchain.schema import Document
    IMPORT_SUCCESS = True
except ImportError as e:
    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å LangChain: {e}")
    logger.warning("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ä–µ–∂–∏–º –±–µ–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞")
    IMPORT_SUCCESS = False

class SimpleKnowledgeBase:
    """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    
    def __init__(self, persist_directory: str = None):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É–ø—Ä–æ—â–µ–Ω–Ω–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        if persist_directory is None:
            current_dir = Path(__file__).parent.parent.parent
            self.persist_directory = str(current_dir / "data" / "vector_db")
        else:
            self.persist_directory = persist_directory
        
        os.makedirs(self.persist_directory, exist_ok=True)
        self.documents = []
        self.load_existing_documents()
    
    def load_existing_documents(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã"""
        data_dir = Path(__file__).parent.parent.parent / "data" / "texts"
        if data_dir.exists():
            text_files = list(data_dir.glob("*.txt"))
            for file_path in text_files:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        text = f.read()
                    
                    if text.strip():
                        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏ (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç)
                        chunks = self.simple_split_text(text)
                        for i, chunk in enumerate(chunks):
                            self.documents.append({
                                "content": chunk,
                                "source": file_path.name,
                                "chunk": i,
                                "metadata": {"type": "law"}
                            })
                        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∏–∑ {file_path.name}")
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {file_path.name}: {e}")
        
        logger.info(f"–í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.documents)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
    
    def simple_split_text(self, text: str, chunk_size: int = 1000) -> List[str]:
        """–ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞—Å—Ç–∏"""
        chunks = []
        start = 0
        text_length = len(text)
        
        while start < text_length:
            end = start + chunk_size
            if end < text_length:
                # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞–∑–±–∏—Ç—å –ø–æ –≥—Ä–∞–Ω–∏—Ü–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                for break_char in ['. ', '\n\n', '\n', ' ']:
                    break_pos = text.rfind(break_char, start, end)
                    if break_pos != -1:
                        end = break_pos + len(break_char)
                        break
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end
        
        return chunks
    
    def search(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:
        """–ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        query_lower = query.lower()
        results = []
        
        for doc in self.documents:
            content_lower = doc["content"].lower()
            # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Å—á–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π —Å–ª–æ–≤
            score = 0
            query_words = query_lower.split()
            for word in query_words:
                if len(word) > 3 and word in content_lower:
                    score += 1
            
            if score > 0:
                # –°–æ–∑–¥–∞–µ–º Document –æ–±—ä–µ–∫—Ç –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                langchain_doc = Document(
                    page_content=doc["content"],
                    metadata={
                        "source": doc["source"],
                        "chunk": doc["chunk"],
                        **doc.get("metadata", {})
                    }
                )
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º score
                normalized_score = min(score / len(query_words), 1.0) if query_words else 0
                results.append((langchain_doc, normalized_score))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        results.sort(key=lambda x: x[1], reverse=True)
        return results[:k]
    
    def get_document_count(self) -> int:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        return len(self.documents)
    
    def get_db_path(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ –ë–î"""
        return self.persist_directory

class AdvancedKnowledgeBase(SimpleKnowledgeBase):
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º –ø–æ–∏—Å–∫–æ–º"""
    
    def __init__(self, persist_directory: str = None):
        if not IMPORT_SUCCESS:
            logger.warning("–ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ LangChain –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ—Å—Ç–æ–π —Ä–µ–∂–∏–º.")
            super().__init__(persist_directory)
            return
        
        if persist_directory is None:
            current_dir = Path(__file__).parent.parent.parent
            self.persist_directory = str(current_dir / "data" / "vector_db")
        else:
            self.persist_directory = persist_directory
        
        os.makedirs(self.persist_directory, exist_ok=True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        try:
            self.embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
            logger.info("–ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
            logger.warning("–ü–µ—Ä–µ—Ö–æ–∂—É –≤ —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ä–µ–∂–∏–º")
            super().__init__(persist_directory)
            self.embeddings = None
            return
        
        self.vectorstore = None
        self._load_or_create_vectorstore()
    
    def _load_or_create_vectorstore(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ"""
        try:
            if os.path.exists(os.path.join(self.persist_directory, "chroma.sqlite3")):
                logger.info(f"–ó–∞–≥—Ä—É–∂–∞—é –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î –∏–∑ {self.persist_directory}")
                self.vectorstore = Chroma(
                    persist_directory=self.persist_directory,
                    embedding_function=self.embeddings
                )
                count = self.vectorstore._collection.count()
                logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            else:
                logger.info("–í–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –°–æ–∑–¥–∞–º –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –ø–æ–∏—Å–∫–µ.")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î: {e}")
            self.vectorstore = None
    
    def search(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:
        """–ü–æ–∏—Å–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î"""
        if self.vectorstore is None or self.embeddings is None:
            logger.warning("–í–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É—é –ø—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫.")
            return super().search(query, k)
        
        try:
            results = self.vectorstore.similarity_search_with_relevance_scores(query, k=k)
            return results
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}. –ò—Å–ø–æ–ª—å–∑—É—é –ø—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫.")
            return super().search(query, k)
    
    def create_vectorstore_from_documents(self):
        """–°–æ–∑–¥–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        if self.embeddings is None:
            logger.error("–ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return False
        
        data_dir = Path(__file__).parent.parent.parent / "data" / "texts"
        if not data_dir.exists():
            logger.error(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {data_dir}")
            return False
        
        text_files = list(data_dir.glob("*.txt"))
        if not text_files:
            logger.error("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤")
            return False
        
        documents = []
        for file_path in text_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    text = f.read()
                
                if text.strip():
                    doc = Document(
                        page_content=text,
                        metadata={
                            "source": file_path.name,
                            "file_path": str(file_path),
                            "type": "law"
                        }
                    )
                    documents.append(doc)
                    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω: {file_path.name} ({len(text)} chars)")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {file_path.name}: {e}")
        
        if not documents:
            logger.error("–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            return False
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", " ", ""]
        )
        
        chunks = text_splitter.split_documents(documents)
        logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∏–∑ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        try:
            self.vectorstore = Chroma.from_documents(
                documents=chunks,
                embedding=self.embeddings,
                persist_directory=self.persist_directory
            )
            self.vectorstore.persist()
            logger.info(f"–í–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î —Å–æ–∑–¥–∞–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {self.persist_directory}")
            return True
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î: {e}")
            return False

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ–º –ø–æ–¥—Ö–æ–¥—è—â–∏–π –∫–ª–∞—Å—Å
try:
    # –ü—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—É—é –≤–µ—Ä—Å–∏—é
    KnowledgeBase = AdvancedKnowledgeBase
    logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º –ø–æ–∏—Å–∫–æ–º")
except Exception as e:
    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—É—é –±–∞–∑—É: {e}")
    logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π")
    KnowledgeBase = SimpleKnowledgeBase

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
def test_knowledge_base():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π...")
    print("=" * 60)
    
    kb = KnowledgeBase()
    print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {kb.get_document_count()}")
    print(f"üìÅ –ü—É—Ç—å –∫ –ë–î: {kb.get_db_path()}")
    
    if hasattr(kb, 'vectorstore') and kb.vectorstore is not None:
        print("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫")
    else:
        print("‚ÑπÔ∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º")
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    test_queries = [
        "–æ—Ç–ø—É—Å–∫ —Ä–∞–±–æ—Ç–Ω–∏–∫–∞",
        "–æ—Ö—Ä–∞–Ω–∞ —Ç—Ä—É–¥–∞",
        "—Ä–∞–±–æ—á–µ–µ –≤—Ä–µ–º—è",
        "–∑–∞—Ä–∞–±–æ—Ç–Ω–∞—è –ø–ª–∞—Ç–∞"
    ]
    
    for query in test_queries:
        print(f"\nüîç –ó–∞–ø—Ä–æ—Å: '{query}'")
        results = kb.search(query, k=2)
        
        if results:
            for i, (doc, score) in enumerate(results, 1):
                source = doc.metadata.get('source', '–î–æ–∫—É–º–µ–Ω—Ç')
                preview = doc.page_content[:150].replace('\n', ' ')
                print(f"   {i}. [{source}] (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {score:.3f})")
                print(f"      {preview}...")
        else:
            print("   ‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
    
    print("\n" + "=" * 60)
    
    # –ï—Å–ª–∏ —ç—Ç–æ AdvancedKnowledgeBase –∏ –Ω–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î, –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å —Å–æ–∑–¥–∞—Ç—å
    if isinstance(kb, AdvancedKnowledgeBase) and kb.vectorstore is None:
        print("\n‚ö†Ô∏è  –í–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î –Ω–µ —Å–æ–∑–¥–∞–Ω–∞. –•–æ—Ç–∏—Ç–µ —Å–æ–∑–¥–∞—Ç—å?")
        print("   –ó–∞–ø—É—Å—Ç–∏—Ç–µ: python -c 'from app.ai.core import KnowledgeBase; kb = KnowledgeBase(); kb.create_vectorstore_from_documents()'")
    
    print("‚úÖ –¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω!")

if __name__ == "__main__":
    test_knowledge_base()
